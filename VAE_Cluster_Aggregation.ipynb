{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Phase 1: Clustering\n",
    "\n",
    "# Load Fashion-MNIST data\n",
    "(x_train, _), (_, _) = fashion_mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "\n",
    "# Define VAE architecture\n",
    "latent_dim = 10\n",
    "\n",
    "def vae_model():\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(256, activation='relu')(input_img)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dense(latent_dim, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "    decoded = Dense(256, activation='relu')(decoded)\n",
    "    decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "    # VAE model\n",
    "    vae = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    return vae, encoder\n",
    "\n",
    "vae, encoder = vae_model()\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "vae.fit(x_train, x_train, epochs=20, batch_size=128)\n",
    "\n",
    "# Get latent representations of all clients\n",
    "latent_representations = encoder.predict(x_train)\n",
    "\n",
    "# Calculate Jensen-Shannon Divergence between latent embeddings\n",
    "def js_divergence(p, q):\n",
    "    \"\"\"Calculate Jensen-Shannon Divergence\"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (jensenshannon(p, m) + jensenshannon(q, m))\n",
    "\n",
    "# Perform clustering based on JSD\n",
    "threshold = 0.1  # Adjust threshold as needed\n",
    "\n",
    "# Compute Jensen-Shannon Divergence matrix\n",
    "def js_divergence_matrix(latent_representations):\n",
    "    num_clients = len(latent_representations)\n",
    "    divergence_matrix = np.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i+1, num_clients):\n",
    "            divergence = js_divergence(latent_representations[i], latent_representations[j])\n",
    "            divergence_matrix[i, j] = divergence\n",
    "            divergence_matrix[j, i] = divergence\n",
    "    return divergence_matrix\n",
    "\n",
    "# Define a function to assign clients to clusters based on divergence matrix and threshold\n",
    "def assign_clusters(divergence_matrix, threshold):\n",
    "    num_clients = len(divergence_matrix)\n",
    "    clusters = [[] for _ in range(num_clients)]\n",
    "    visited = set()\n",
    "    for i in range(num_clients):\n",
    "        if i not in visited:\n",
    "            visited.add(i)\n",
    "            clusters[i].append(i)\n",
    "            for j in range(i+1, num_clients):\n",
    "                if j not in visited and divergence_matrix[i, j] < threshold:\n",
    "                    visited.add(j)\n",
    "                    clusters[i].append(j)\n",
    "                    clusters[j].append(i)\n",
    "    return clusters\n",
    "\n",
    "# Compute divergence matrix\n",
    "divergence_matrix = js_divergence_matrix(latent_representations)\n",
    "# Assign clusters based on divergence matrix and threshold\n",
    "clusters = assign_clusters(divergence_matrix, threshold)\n",
    "\n",
    "# Determine cluster heads\n",
    "cluster_heads = {}\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_points = latent_representations[cluster]\n",
    "    cluster_head_index = np.argmax(np.linalg.norm(cluster_points, axis=1))\n",
    "    cluster_heads[i] = cluster_points[cluster_head_index]\n",
    "\n",
    "# Phase 2: Global Training\n",
    "\n",
    "# Define SimpleMLP class for local training\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "\n",
    "# Perform local training using SimpleMLP\n",
    "def local_training(x_train, y_train, num_classes):\n",
    "    shape = x_train.shape[1]\n",
    "    model = SimpleMLP.build(shape, num_classes)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0)  # Adjust epochs and batch_size as needed\n",
    "    return model\n",
    "\n",
    "# Define aggregation weightage calculation function\n",
    "def calculate_aggregation_weight(alpha, beta, gamma, latent_values, num_classes, num_samples, total_samples, total_classes, total_latent):\n",
    "    weight = (1 - alpha - beta - gamma) * (np.linalg.norm(latent_values) / np.linalg.norm(total_latent))\n",
    "    weight += (1 - alpha - beta) * (num_classes / total_classes)\n",
    "    weight += (1 - alpha) * (num_samples / total_samples)\n",
    "    weight += alpha\n",
    "    return weight\n",
    "\n",
    "# Perform cluster aggregation with aggregation weightage\n",
    "def cluster_aggregation(cluster, trained_models, alpha, beta, gamma):\n",
    "    num_samples = sum([x_train.shape[0] for x_train, _ in cluster])\n",
    "    num_classes = trained_models[0].output_shape[1]\n",
    "    latent_values = sum([np.mean(encoder.predict(x_train), axis=0) for x_train, _ in cluster]) / len(cluster)\n",
    "    total_samples = sum(num_samples for _, _ in cluster)\n",
    "    total_classes = sum(num_classes for _, _ in cluster)\n",
    "    total_latent = sum(latent_values)\n",
    "    \n",
    "    cluster_models_weighted = []\n",
    "    for x_train, y_train in cluster:\n",
    "        model = local_training(x_train, y_train, num_classes)\n",
    "        weight = calculate_aggregation_weight(alpha, beta, gamma, latent_values, num_classes, x_train.shape[0], total_samples, total_classes, total_latent)\n",
    "        cluster_models_weighted.append((model, weight))\n",
    "        \n",
    "    # Perform weighted aggregation\n",
    "    aggregated_model = SimpleMLP.build(trained_models[0].input_shape[1], trained_models[0].output_shape[1])\n",
    "    for model, weight in cluster_models_weighted:\n",
    "        # Aggregation method using weight\n",
    "        # aggregated_model.add(model * weight)  # Example aggregation (not actual aggregation method)\n",
    "        pass\n",
    "    # Compile the aggregated model\n",
    "    aggregated_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return aggregated_model\n",
    "\n",
    "# Perform global aggregation with aggregation weightage\n",
    "def global_aggregation(cluster_models, alpha, beta, gamma):\n",
    "    total_samples = sum(sum(x_train.shape[0] for x_train, _ in cluster) for cluster in cluster_models)\n",
    "    total_classes = sum(cluster[0].output_shape[1] for cluster in cluster_models)\n",
    "    total_latent = sum(np.mean(encoder.predict(x_train), axis=0) for cluster in cluster_models for x_train, _ in cluster)\n",
    "    \n",
    "    global_models_weighted = []\n",
    "    for cluster in cluster_models:\n",
    "        weight = calculate_aggregation_weight(alpha, beta, gamma, total_latent, total_classes, total_samples, total_samples, total_classes, total_latent)\n",
    "        global_models_weighted.append(cluster * weight)\n",
    "        \n",
    "    # Perform weighted aggregation\n",
    "    global_model = SimpleMLP.build(cluster_models[0].input_shape[1], cluster_models[0].output_shape[1])\n",
    "    for model, weight in global_models_weighted:\n",
    "        # Aggregation method using weight\n",
    "        # global_model.add(model * weight)  # Example aggregation (not actual aggregation method)\n",
    "        pass\n",
    "    # Compile the global model\n",
    "    global_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return global_model\n",
    "\n",
    "\n",
    "# Cluster aggregation\n",
    "alpha = 0.333\n",
    "beta = 0.333\n",
    "gamma = 0.333\n",
    "aggregated_cluster_model = cluster_aggregation(clusters, [], alpha, beta, gamma)\n",
    "\n",
    "# Global aggregation\n",
    "aggregated_global_model = global_aggregation([aggregated_cluster_model], alpha, beta, gamma)\n",
    "\n",
    "# Evaluate the aggregated global model\n",
    "loss, accuracy = aggregated_global_model.evaluate(x_train, y_train)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
